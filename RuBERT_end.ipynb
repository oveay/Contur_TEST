{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RuBERT_end.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4Ehvp10xtRusZ66Fv9bjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oveay/Contur_TEST/blob/main/RuBERT_end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели на основе эмбеддингов\n",
        "\n",
        "Необходимо преобразовать текстовые данные, чтобы можно было подать их на вход модели. Можно разными способами закодировать слова, либо символы, создать словарь токенов и заменить каждый токен на его номер. Но в таком случае наша модель не будет иметь информации о самих словах и что они значат в языке.\n",
        "\n",
        "Первым решением данной проблемы могут стать предобученные эмбеддинги для слов (например fasttext), тогда модель будет иметь представление о каждом отдельно взятом токене, но не будет знать ничего о слове в конкретном контексте входных данных. Можно было бы сверху на этих эмбеддингах обучить, например biderectional LSTM и таким образом получить представление предложений, но в данном блокноте рассматривается другой подход.\n",
        "\n",
        "С помощью предобученной модели - RuBERT, будут получены векторные представления предложений, которые уже хранят информацию о нем, а далее, поверх новых эмбеддингов, будут обучены модели классификации на исходную задачу."
      ],
      "metadata": {
        "id": "lpjUs_YKkWd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hnaiO3HeMar"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Библиотека с предобученными трансформерами\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Для разбиения train выборки\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Библиотеки для классификации\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Для подсчета метрик\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score"
      ],
      "metadata": {
        "id": "1JEzr2NKeVlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Константы\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_FILENAME = 'train.tsv'\n",
        "TEST_FILENAME = 'test.tsv'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "bUDD-gBMgQ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Считывание данных"
      ],
      "metadata": {
        "id": "W8LWzVOTyqb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_tsv(filename):\n",
        "  df = pd.read_csv(filename, sep=\"\\t\")\n",
        "  return (df[i] for i in df) # Возвращаем итератор столбцов датафрейма"
      ],
      "metadata": {
        "id": "IkL1KjtXhc0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = read_tsv(TRAIN_FILENAME) # Распаковываем итератор\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "zx7fc9TmgfPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация данных"
      ],
      "metadata": {
        "id": "f5zO5KRey-qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(data):\n",
        "  tokenized = tokenizer(list(data), padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
        "  tokenized = tokenized.to(DEVICE)\n",
        "  return tokenized"
      ],
      "metadata": {
        "id": "YSuDAaznocD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
      ],
      "metadata": {
        "id": "g7VyerrVrmoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = get_tokens(X_train)\n",
        "tokenized_val = get_tokens(X_val)"
      ],
      "metadata": {
        "id": "iq_CWurXhL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Получение эмбеддингов предложений"
      ],
      "metadata": {
        "id": "0J3TQSjczTM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание предобученной модели\n",
        "model_RuBERT = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\").to(DEVICE)"
      ],
      "metadata": {
        "id": "lPDxLZ8QiVqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Данная функция взята с https://huggingface.co '''\n",
        "\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "s7KyoK7hkXd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embeddings(tokenized):\n",
        "  with torch.no_grad():\n",
        "    model_output = model_RuBERT(**tokenized)\n",
        "  \n",
        "  return mean_pooling(model_output, tokenized['attention_mask'])\n",
        "\n",
        "\n",
        "train_embeddings = get_sentence_embeddings(tokenized_train)\n",
        "val_embeddings = get_sentence_embeddings(tokenized_val)"
      ],
      "metadata": {
        "id": "ovjlQEegq6l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Построение моделей классификации"
      ],
      "metadata": {
        "id": "MzyTg-CT0mcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_models = dict()\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), SVC(kernel='rbf', gamma='scale'))\n",
        "clf.fit(train_embeddings.cpu(), y_train)\n",
        "classification_models['SVM'] = clf\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
        "clf.fit(train_embeddings.cpu(), y_train)\n",
        "classification_models['RandomForestClassifier'] = clf\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "clf.fit(train_embeddings.cpu(), y_train)\n",
        "classification_models['LogisticRegeression'] = clf\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), xgb.XGBClassifier())\n",
        "clf.fit(train_embeddings.cpu(), y_train)\n",
        "classification_models['GBM'] = clf"
      ],
      "metadata": {
        "id": "itNaWZedkM0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подсчет метрик"
      ],
      "metadata": {
        "id": "dB_Jp2a-01TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "  metric_scores = dict()\n",
        "\n",
        "  metric_scores['f1_score'] = f1_score(y_true, y_pred)\n",
        "  metric_scores['roc-auc'] = roc_auc_score(y_true, y_pred)\n",
        "  metric_scores['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "  return metric_scores"
      ],
      "metadata": {
        "id": "5AwXttEzn3XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, class_model in classification_models.items():\n",
        "  y_pred = np.array(class_model.predict(val_embeddings.cpu()))\n",
        "  metrics = compute_metrics(y_val, y_pred)\n",
        "\n",
        "  print(f'{model_name}:')\n",
        "  for score_name, score in metrics.items():\n",
        "    print(f'{score_name}: {score}')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0JuY-ZdqoUN",
        "outputId": "19043076-75fb-4bf3-a700-5b99e5a92838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM:\n",
            "f1_score: 0.9084687767322498\n",
            "roc-auc: 0.9072953736654805\n",
            "accuracy: 0.9071180555555556\n",
            "\n",
            "RandomForestClassifier:\n",
            "f1_score: 0.8568980291345331\n",
            "roc-auc: 0.8552234754810302\n",
            "accuracy: 0.8550347222222222\n",
            "\n",
            "LogisticRegeression:\n",
            "f1_score: 0.874251497005988\n",
            "roc-auc: 0.8725526268170577\n",
            "accuracy: 0.8723958333333334\n",
            "\n",
            "GBM:\n",
            "f1_score: 0.8688245315161841\n",
            "roc-auc: 0.866367090898124\n",
            "accuracy: 0.8663194444444444\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Анализ\n",
        "\n",
        "Наилучший результат показывает SVM, в районе 0.9 - 0.91 по каждой из трех метрик, поэтому именно этот тип модели классификации мы будем использовать для разметки тестовых данных."
      ],
      "metadata": {
        "id": "PfbzqWqgnAEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение полной модели\n",
        "\n",
        "В предыдущих разделах мы разделяли данную нам тренировочную выборку на непосредственно train и validation, чтобы выбрать модель классификации и получить значения метрик.\n",
        "\n",
        "Но основная задача стоит в разметке данных из файла 'test.tsv', поэтому нет смысла обучать модель лишь на часте тренировочных данных"
      ],
      "metadata": {
        "id": "XLeujgaSnfGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X, y - полные данные, которые мы сплитовали\n",
        "\n",
        "tokenized_data = get_tokens(X)\n",
        "data_embeddings = get_sentence_embeddings(tokenized_data)\n",
        "\n",
        "classificator = make_pipeline(StandardScaler(), SVC(kernel='rbf', gamma='scale'))\n",
        "classificator.fit(data_embeddings.cpu(), y)"
      ],
      "metadata": {
        "id": "6FJSKBAktGbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, _ = read_tsv(TEST_FILENAME) # y - не нужен, так как это неразмеченные данные\n",
        "\n",
        "tokenized_test = get_tokens(X_test)\n",
        "test_embeddings = get_sentence_embeddings(tokenized_test)\n",
        "\n",
        "y_predictions = classificator.predict(test_embeddings.cpu())"
      ],
      "metadata": {
        "id": "_v6LzHptvkpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Запись данных в файл"
      ],
      "metadata": {
        "id": "vpUG3-zU9HgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pd.DataFrame({'title': X_test, 'is_fake': y_predictions})\n",
        "predictions.set_index('title', inplace=True)"
      ],
      "metadata": {
        "id": "_Nor4XAq1eM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.to_csv('predictions.tsv', sep=\"\\t\")"
      ],
      "metadata": {
        "id": "ArVtnPNj4tdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}